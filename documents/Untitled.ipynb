{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce6dd6addb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m  \u001b[0;31m# dynamically grow the memory used on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_device_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m  \u001b[0;31m# to log device placement (on which device the operation ran)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation informaiton\n",
    "f= open('/home/dkkim/downloads/tiny-imagenet-200/original/val/val_annotations.txt')\n",
    "\n",
    "array=[]\n",
    "\n",
    "for line in f.readlines():\n",
    "    array.append([])\n",
    "    for i in line.split():\n",
    "        array[-1].append(i)\n",
    "\n",
    "print len(array)\n",
    "# print array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate val images\n",
    "import os, shutil\n",
    "\n",
    "# original_dataset_dir = '/home/dkkim/downloads/tiny-imagenet-200/original/val/images'\n",
    "# train_dir = '/home/dkkim/downloads/tiny-imagenet-200/train'\n",
    "# val_dir = '/home/dkkim/downloads/tiny-imagenet-200/val'\n",
    "\n",
    "# base_dir = '/home/dkkim/downloads/tiny-imagenet-200/val'\n",
    "# if not os.path.exists(base_dir):os.mkdir(base_dir)\n",
    "    \n",
    "#train,validation,test dir\n",
    "\n",
    "# validation_dir=os.path.join(base_dir,'images')\n",
    "# if not os.path.exists(validation_dir): os.mkdir(validation_dir)\n",
    "    \n",
    "# for i in range(10000):\n",
    "#     dir = os.path.join(base_dir,array[i][1])\n",
    "    \n",
    "#     fname = array[i][0]\n",
    "# #     print array[i][0],array[i][1]\n",
    "#     if not os.path.exists(dir):os.mkdir(dir)\n",
    "#     src=os.path.join(original_dataset_dir,fname)\n",
    "#     dst=os.path.join(dir,fname)\n",
    "#     shutil.copyfile(src,dst)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing\n",
    "\n",
    "train_dir = '/data/tiny-imagenet-200/train'\n",
    "val_dir = '/data/tiny-imagenet-200/val'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,width_shift_range =0.1,height_shift_range=0.1,shear_range=0.1,zoom_range=0.1,horizontal_flip=True,fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator_2 = train_datagen.flow_from_directory(train_dir,target_size=(128,128),batch_size=200,class_mode='categorical')\n",
    "\n",
    "validation_generator_2 = val_datagen.flow_from_directory(val_dir,target_size=(128,128),batch_size=200,class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, merge, Concatenate, concatenate, Input, Dropout, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def resnet(input,n_ch):\n",
    "    x1 = layers.convolutional.ZeroPadding2D((1,1))(input)\n",
    "    x1 = Conv2D(n_ch,(3,3), kernel_initializer='he_normal')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x1 = layers.convolutional.ZeroPadding2D((1,1))(x1)\n",
    "    x1 = Conv2D(n_ch,(3,3), kernel_initializer='he_normal')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    output = concatenate(inputs = [input,x1])\n",
    "    output = layers.Activation('relu')(output)\n",
    "    output = Conv2D(n_ch,(1,1),kernel_initializer='he_normal')(output)\n",
    "    return output\n",
    "\n",
    "# def skip_connect(input,n_ch):\n",
    "    \n",
    "#     x1 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(input)\n",
    "#     x1 = BatchNormalization()(x1)\n",
    "#     x1 = layers.Activation('relu')(x1)\n",
    "#     x1 = layers.convolutional.ZeroPadding2D((1,1))(x1)\n",
    "#     x1 = Conv2D(n_ch,(3,3), kernel_initializer='he_normal')(x1)\n",
    "#     x1 = BatchNormalization()(x1)\n",
    "#     x1 = layers.Activation('relu')(x1)\n",
    "#     x1 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(x1)\n",
    "#     x1 = BatchNormalization()(x1)\n",
    "#     x1 = layers.Activation('relu')(x1)\n",
    "    \n",
    "#     x2 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(input)\n",
    "#     x2 = BatchNormalization()(x2)\n",
    "#     x2 = layers.Activation('relu')(x2)\n",
    "    \n",
    "#     output = concatenate(inputs = [x2,x1])\n",
    "#     return output\n",
    "\n",
    "def skip_connect(input,n_ch):\n",
    "\n",
    "    x1 = BatchNormalization()(input)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x1 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(x1)    \n",
    "    x1 = layers.convolutional.ZeroPadding2D((1,1))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x1 = Conv2D(n_ch,(3,3), kernel_initializer='he_normal')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = layers.Activation('relu')(x1)\n",
    "    x1 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(x1)\n",
    "    \n",
    "\n",
    "    x2 = BatchNormalization()(input)\n",
    "    x2 = layers.Activation('relu')(x2)\n",
    "    x2 = Conv2D(n_ch/2,(1,1), kernel_initializer='he_normal')(x2)    \n",
    "    \n",
    "    output = concatenate(inputs = [x2,x1])\n",
    "    return output\n",
    "    \n",
    "\n",
    "input_dat = Input(shape=(128,128,3))\n",
    "\n",
    "first_layer = layers.convolutional.ZeroPadding2D((1,1))(input_dat)\n",
    "first_layer = BatchNormalization()(first_layer)\n",
    "first_layer = Conv2D(64,(3,3), kernel_initializer='he_normal')(first_layer)\n",
    "# first_layer = layers.Activation('relu')(first_layer)\n",
    "# first_layer = layers.MaxPooling2D((2,2))(first_layer)\n",
    "\n",
    "x1 = skip_connect(first_layer,64)\n",
    "x1 = skip_connect(x1,64)\n",
    "x1 = skip_connect(x1,64)\n",
    "x1 = layers.MaxPooling2D((2,2))(x1)\n",
    "\n",
    "x2 = skip_connect(x1,128)\n",
    "x2 = skip_connect(x2,128)\n",
    "x2 = skip_connect(x2,128)\n",
    "x2 = layers.MaxPooling2D((2,2))(x2)\n",
    "\n",
    "x2_2 = skip_connect(x2,256)\n",
    "x2_2 = skip_connect(x2_2,256)\n",
    "x2_2 = skip_connect(x2_2,256)\n",
    "x2_2 = layers.MaxPooling2D((2,2))(x2)\n",
    "\n",
    "x3 = skip_connect(x2_2,512)\n",
    "x3 = skip_connect(x3,512)\n",
    "x3 = skip_connect(x3,512)\n",
    "x3 = layers.MaxPooling2D((2,2))(x3)\n",
    "\n",
    "x4 = skip_connect(x3,1024)\n",
    "x4 = skip_connect(x4,1024)\n",
    "x4 = skip_connect(x4,1024)\n",
    "\n",
    "\n",
    "output =  layers.AveragePooling2D((8,8))(x4)\n",
    "output = layers.Flatten()(output)\n",
    "output = layers.Dense(200,kernel_regularizer=regularizers.l2(0.001),activation='softmax', kernel_initializer='he_normal')(output)\n",
    "# output = layers.Dense(200,activation='relu', kernel_initializer='he_normal')(output)\n",
    "\n",
    "model=models.Model(inputs=input_dat,outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler,Callback\n",
    "import os, shutil\n",
    "import math\n",
    "\n",
    "# learning rate log\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "## learning rate decay\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.1\n",
    "    epochs_drop = 10\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop)) # lr = lr0 * drop^floor(epoch / epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self,batch,logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "\n",
    "## model compile\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.001)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc',lr_metric])\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "# check point\n",
    "log_dir='/data/tiny_3_log'\n",
    "log_data_dir=os.path.join(log_dir,'log_3') ################ version\n",
    "if not os.path.exists(log_data_dir): os.mkdir(log_data_dir)\n",
    "    \n",
    "filepath=os.path.join(log_data_dir,'weights.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint,lrate]\n",
    "\n",
    "\n",
    "#training\n",
    "history=model.fit_generator(train_generator_2,steps_per_epoch=1250,epochs=30,callbacks=callbacks_list,validation_data=validation_generator_2,validation_steps=50)\n",
    "\n",
    "\n",
    "\n",
    "# model.save\n",
    "\n",
    "model_weight=os.path.join(log_data_dir,'tiny_3_imagenet.h5')\n",
    "model_arch=os.path.join(log_data_dir,'tiny_3_imagenet.json')\n",
    "\n",
    "model.save(model_weight)\n",
    "with open(model_arch,'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "import csv\n",
    "\n",
    "dict = history.history\n",
    "\n",
    "csv_path=os.path.join(log_data_dir,'tiny_3_log.csv')\n",
    "\n",
    "w = csv.writer(open(csv_path, \"w\"))\n",
    "for key, val in dict.items():\n",
    "    w.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save\n",
    "\n",
    "model_weight=os.path.join(log_data_dir,'tiny_2_imagenet.h5')\n",
    "model_arch=os.path.join(log_data_dir,'tiny_2_imagenet.json')\n",
    "\n",
    "model.save(model_weight)\n",
    "with open(model_arch,'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "import csv\n",
    "\n",
    "dict = history.history\n",
    "\n",
    "csv_path=os.path.join(log_data_dir,'tiny_2_log.csv')\n",
    "\n",
    "w = csv.writer(open(csv_path, \"w\"))\n",
    "for key, val in dict.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc,'bo',label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc,'b',label = 'Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label = 'Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# with open('/home/dkkim/documents/tiny_log_2.txt','w') as output:\n",
    "#     output.write(str(history.history))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os, shutil\n",
    "\n",
    "# path_check='/data/tiny-imagenet-200/train/n01644900/images'\n",
    "# fnames = sorted([os.path.join(path_check, fname) for fname in os.listdir(path_check)])\n",
    "\n",
    "\n",
    "# img_path = fnames[9]\n",
    "\n",
    "# img = image.load_img(img_path, target_size=(64,64))\n",
    "\n",
    "# x = image.img_to_array(img)\n",
    "# x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# datagen = ImageDataGenerator(rotation_range=10,width_shift_range =0.1,height_shift_range=0.1,shear_range=0.1,zoom_range=0.1,horizontal_flip=True,fill_mode='nearest')\n",
    "\n",
    "# j=0\n",
    "# for batch in datagen.flow(x, batch_size =1):\n",
    "#     plt.figure(j)\n",
    "#     imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "#     j += 1\n",
    "#     if j%4 ==0:\n",
    "#         break\n",
    "        \n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
